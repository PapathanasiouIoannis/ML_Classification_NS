{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c66c4ff-7fe5-4b4c-bfcf-5cc9ff2b09d2",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Grid Search)\n",
    "\n",
    "## Role in the Project\n",
    "This script is the **optimization engine** for the machine learning pipeline. Its purpose is to systematically search for the most effective configuration of the Random Forest classifiers, ensuring that the final models achieve maximum accuracy without overfitting to the training data.\n",
    "\n",
    "It acknowledges that different input feature sets require different learning strategies. Therefore, it conducts separate optimization routines for:\n",
    "1.  **Observational Models (Model A/Geo):** Which rely on limited, highly correlated features (Mass and Radius).\n",
    "2.  **Physics Models (Model D):** Which have access to rich, decorrelated microphysical features (Energy Density, Sound Speed, Slope).\n",
    "\n",
    "## Physics and Equations\n",
    "\n",
    "### 1. The Bias-Variance Tradeoff\n",
    "The optimization process navigates the Bias-Variance tradeoff to minimize the Generalization Error $E_{gen}$. The total error can be decomposed as:\n",
    "\n",
    "$$\n",
    "E_{gen} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "$$\n",
    "\n",
    "*   **Bias:** Error due to erroneous assumptions (e.g., a tree that is too shallow to capture the complex EoS topology).\n",
    "*   **Variance:** Error due to sensitivity to small fluctuations in the training set (e.g., a tree that is too deep and memorizes noise).\n",
    "\n",
    "The hyperparameters tuned here control this balance:\n",
    "*   `max_depth`: Limits the complexity of the decision boundary (reducing Variance).\n",
    "*   `min_samples_leaf`: Enforces smoothness by requiring a minimum number of stars in each leaf node.\n",
    "*   `max_features`: Controls the correlation between trees in the forest.\n",
    "\n",
    "### 2. Feature Correlation Strategy\n",
    "A critical physical insight drives the tuning strategy:\n",
    "*   **Mass and Radius are strongly correlated.** In a typical neutron star sequence, determining the mass strongly constrains the radius. If the model is forced to pick only one feature at a split (`max_features='sqrt'`), it might lose context. Therefore, Observational models are tested with `max_features=None` (seeing all features).\n",
    "*   **Microphysics features are distinct.** Central density $\\epsilon_c$ and sound speed $c_s^2$ provide orthogonal information to the macroscopic observables. Physics models benefit from `max_features='sqrt'` to force trees to learn these independent physical drivers.\n",
    "\n",
    "## Algorithm and Calculations\n",
    "\n",
    "1.  **Data Preparation**\n",
    "    *   The dataset is loaded and cleaned.\n",
    "    *   Logarithmic transformations are applied to the Tidal Deformability ($\\Lambda$) to stabilize its wide dynamic range.\n",
    "    *   Stars are grouped by their `Curve_ID`. This is crucial for the cross-validation strategy.\n",
    "\n",
    "2.  **Group K-Fold Cross-Validation**\n",
    "    Standard random splitting is scientifically invalid here because stars from the same Equation of State (EoS) curve are physically related. If stars from Curve X are in both the training and validation sets, the model will \"memorize\" Curve X rather than learning general physics.\n",
    "    To prevent this leakage, a **Group K-Fold** splitter is used:\n",
    "    *   The dataset is divided into $K=3$ folds.\n",
    "    *   **Constraint:** All stars belonging to a specific EoS curve must appear in the same fold.\n",
    "    *   The model is trained on $K-1$ folds and validated on the remaining fold.\n",
    "\n",
    "3.  **Grid Search Execution**\n",
    "    Two distinct grid searches are performed:\n",
    "\n",
    "    *   **Scenario A (Observational):**\n",
    "        *   Features: Mass, Radius, $\\log\\Lambda$.\n",
    "        *   Hypothesis: The model needs to see all features simultaneously to resolve the $M-R$ degeneracy.\n",
    "        *   Grid: Tests `max_features` options (`sqrt` vs `None`) and regularization depths.\n",
    "\n",
    "    *   **Scenario D (Microphysics):**\n",
    "        *   Features: Mass, Radius, $\\log\\Lambda$, $\\epsilon_c$, $c_s^2$, Slope.\n",
    "        *   Hypothesis: The model should be forced to decorrelate trees to exploit the rich feature set.\n",
    "        *   Constraint: `max_features` is locked to `'sqrt'`.\n",
    "\n",
    "4.  **Metric Evaluation**\n",
    "    For every combination of hyperparameters, the average validation accuracy across the 3 folds is computed. The set of parameters that yields the highest accuracy is reported as the \"Best Params.\"\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "### Inputs\n",
    "*   **Dataset:** The `thesis_dataset.csv` file containing the full population of simulated stars.\n",
    "\n",
    "### Outputs\n",
    "*   **Optimization Report:** A printed summary to the console detailing:\n",
    "    *   The best hyperparameter set for Model A.\n",
    "    *   The best hyperparameter set for Model D.\n",
    "    *   The maximum cross-validation accuracy achieved for each.\n",
    "\n",
    "## Connection to the Overall Workflow\n",
    "This script is usually run **offline** or **intermittently**. It does not need to run every time the pipeline executes.\n",
    "\n",
    "Its results (the \"Best Params\") are manually transferred to `train_model.py` to hardcode the configuration of the final classifiers. This ensures that the production models are always using the scientifically optimal settings derived from this rigorous search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1cefd2-6912-4aaa-8672-cae6eba0d3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
