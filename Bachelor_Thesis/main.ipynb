{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd58d6fa-fd78-432f-bd60-8ecc08e4a7ca",
   "metadata": {},
   "source": [
    "# Master Pipeline Orchestrator\n",
    "\n",
    "## Detailed Analysis\n",
    "\n",
    "This script serves as the central entry point for the entire project. It orchestrates the workflow, which consists of four main stages: synthetic data generation, machine learning model training, comprehensive visualization, and inference on real astrophysical candidates.\n",
    "\n",
    "The script manages the lifecycle of the dataset. If a valid dataset (`data/thesis_dataset.csv`) exists, it is loaded; otherwise, a parallelized simulation routine is triggered to generate thousands of Equations of State (EoS) for both Hadronic and Quark stars. It ensures class balance between the two populations before passing the data to the machine learning pipeline and the visualization suite.\n",
    "\n",
    "## Physics and Math\n",
    "\n",
    "While this script primarily handles logic and data flow, it enforces statistical balance in the dataset. To prevent bias in the machine learning models, the dataset is undersampled such that:\n",
    "\n",
    "$$\n",
    "N_{samples} = \\min(N_{hadronic}, N_{quark})\n",
    "$$\n",
    "\n",
    "This ensures that the priors for the classification task are balanced ($P(H) \\approx P(Q) \\approx 0.5$).\n",
    "\n",
    "The script also utilizes the **homologous scaling** baseline masses calculated in `calculate_baselines.py`, which are essential for the \"Inverse Sampling\" technique used in the hadronic generation workers.\n",
    "\n",
    "## Code Walkthrough\n",
    "\n",
    "### 1. Configuration and Initialization\n",
    "The script begins by setting up the necessary directories (`data/` and `plots/`) and loading the physical constants and baselines.\n",
    "\n",
    "```python\n",
    "# Directory Setup\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Physics Initialization\n",
    "baselines = calculate_baselines()\n",
    "```\n",
    "The `baselines` dictionary contains the maximum stable masses for pure hadronic models, which are required inputs for the worker functions.\n",
    "\n",
    "### 2. Data Management and Generation\n",
    "The script checks for an existing dataset. If the file is missing or the schema does not match the current configuration, a parallel generation process is initiated using `joblib`.\n",
    "\n",
    "```python\n",
    "if should_generate:\n",
    "    # Define tasks (interleaved Hadronic and Quark batches)\n",
    "    for i in range(num_batches):\n",
    "        t_type = 'hadronic' if i % 2 == 0 else 'quark'\n",
    "        tasks.append((t_type, CURVES_PER_BATCH, i, i))\n",
    "\n",
    "    # Execute Parallel Workers\n",
    "    res = Parallel(n_jobs=N_JOBS)(\n",
    "        delayed(run_worker_wrapper)(t, baselines) for t in tqdm(tasks)\n",
    "    )\n",
    "```\n",
    "This block distributes the computational load across all available CPU cores. Each task generates a batch of TOV sequences.\n",
    "\n",
    "### 3. Class Balancing\n",
    "After generation, the results are consolidated into a pandas DataFrame. The script then balances the classes to ensure the machine learning models are not biased toward the class that was easier to generate (typically Quark stars).\n",
    "\n",
    "```python\n",
    "# Undersample the majority class\n",
    "min_count = counts.min()\n",
    "df = df.groupby('Label').sample(n=min_count, random_state=42)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "```\n",
    "\n",
    "### 4. Machine Learning Pipeline\n",
    "The balanced DataFrame is passed to the training module. This function trains the hierarchy of models (Geometric, Model A, B, C, D) and returns the trained objects along with a held-out test set for independent evaluation.\n",
    "\n",
    "```python\n",
    "models_dict, X_test, y_test = train_model(df)\n",
    "```\n",
    "\n",
    "### 5. Visualization and Inference Suite\n",
    "Finally, the script calls a series of specialized plotting functions. These cover everything from ML diagnostics (ROC curves, confusion matrices) to physical insights (Sound speed profiles, M-R diagrams, Stability windows).\n",
    "\n",
    "It also performs inference on real astrophysical objects (e.g., GW170817, PSR J0740+66) using the `analyze_candidates` function.\n",
    "\n",
    "```python\n",
    "# ML Diagnostics\n",
    "plot_diagnostics(models_dict, X_test, y_test)\n",
    "\n",
    "# Physics Manifolds\n",
    "plot_grand_summary(df)\n",
    "plot_physics_manifold(df)\n",
    "\n",
    "# Inference on Real Data\n",
    "analyze_candidates(models_dict)\n",
    "```\n",
    "\n",
    "## Visualization Output\n",
    "\n",
    "This script does not produce plots directly but acts as the trigger for the entire visualization suite. Upon successful execution, the `plots/` directory will contain:\n",
    "\n",
    "*   **ML Diagnostics**: ROC curves, calibration plots, and learning curves.\n",
    "*   **Physics Manifolds**: Mass-Radius diagrams, Tidal Deformability plots, and EoS comparisons.\n",
    "*   **Microphysics**: 3D phase space plots, stability windows, and sound speed correlations.\n",
    "*   **Interpretability**: Partial Dependence Plots (PDP) and SHAP summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71204e5-9765-472d-94df-c8bb5c91861c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
